# ====== Global config ======
# IMPORTANT: change this to your real token or export it before `docker compose up -d`
HUGGINGFACE_HUB_TOKEN=DEMO_HF_TOKEN_ONLY

# Shared model/cache folders (mounted into all services)
MODELS_ROOT=./models
HF_CACHE=./huggingface

# ====== vLLM (LLM) ======
VLLM_MODEL=Qwen/Qwen3-VL-30B-A3B-Instruct
VLLM_PORT=8000
# Set tensor parallel to your number of GPUs, or leave empty to let vLLM pick 1
VLLM_TP=
VLLM_EXTRA_ARGS="--max-model-len 32768 --dtype auto --trust-remote-code"

# ====== ASR (Canary) ======
ASR_PORT=6000
ASR_WORKERS=1

# ====== Diarization (pyannote) ======
DIAR_PORT=7000

# ====== Admin API + Web UI ======
ADMIN_PORT=8080
